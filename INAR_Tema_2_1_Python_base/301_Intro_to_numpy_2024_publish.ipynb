{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"font-size: 200%; font-weight: bold; color: maroon;\">301_Intro_to_numpy</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Some introduction to numpy\n",
    "\n",
    "numpy (most frequently imported as np) is the linear algebra library for python environments. \n",
    "\n",
    "In order to work -to implement vectorization, which is the basis of the computational advantage of numpy- you have first to define a numpy object: a matrix.\n",
    "\n",
    "Once defined a matrix, ie x, you can simply call vectorized functions using np. syntax.\n",
    "\n",
    "For instance:\n",
    "\n",
    "- np.exp(x) works for any np.array x and applies the exponential function to every coordinate\n",
    "\n",
    "In summary, numpy has efficient built-in functions for computing matrices, it is fast because it vectorizes the computations\n",
    "\n",
    "\n",
    "The following is adapted from the great Andrew Ng's coursera on deep learning\n",
    "\n",
    "## Important numpy links / reference material \n",
    "\n",
    "numpy official user guide (good, but long):  https://numpy.org/doc/stable/user/\n",
    "\n",
    "one simple / quick numpy cheatsheet : https://s3.amazonaws.com/dq-blog-files/numpy-cheat-sheet.pdf\n",
    "\n",
    "This tutorial looks rather useful:\n",
    "https://physics.nyu.edu/pine/pymanual/html/chap3/chap3_arrays.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "source": [
    "# Vectorization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "In machine or deep learning, you deal with very large datasets. Hence, a non-computationally-optimal function can become a huge bottleneck in your algorithm and can result in a model that takes ages to run. To make sure that your code is  computationally efficient, you will use vectorization. For example, try to tell the difference between the following implementations of the dot/outer/elementwise product."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import random\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El objeto básico de numpy son las matrices. Las matrices son colecciones multidimensionales, esto es, pueden tener entre 1 y n dimensiones. \n",
    "\n",
    "## PROPIEDADES BÁSICAS DE LAS MATRICES:\n",
    "\n",
    "- Cada dimensión de una matriz se puede definir como una lista, y de hecho una matriz numpy será una lista de listas (dobles/triples... [])pero:\n",
    "- Todos los elementos de la matriz deben ser del mismo tipo (entero, float -sería raro pero es posible que fueran strings o booleanos)\n",
    "- Todas las listas (las \"columnas\" de esa matriz/tabla multidimensional) deben tener el mismo número de elementos. Si falta alguno debería contener np.nan (NaN, not a number)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "matriz1 = np.array([[1., 0., 0.],\n",
    "                    [0., 1., 2.]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "matriz1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "matriz1.ndim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unos = np.ones((2, 3, 4), dtype=np.int16)\n",
    "unos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aleat1 = np.random.rand(2, 3, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aleat1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aleat1[0, 2, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = np.array([[1, 2], [5, 3], [4, 6]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Índices de numpy arrays son diferentes de los de objetos (tablas) pandas, y son de hecho un poco confusos. Para una buena explicación:\n",
    "\n",
    "https://www.sharpsightlabs.com/blog/numpy-axes-explained/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.max(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.max(axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "source": [
    "## Check the advantages of vectorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x1 = [random.random() for e in range(10**4)]  # remember list comprehension\n",
    "x2 = [random.random() for e in range(10**4)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(type(x1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(x1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the next example code, we will make basic matrix computations **without** numpy, this is using classical for loops. \n",
    "\n",
    "Later we will do the same but using vectorization, ie, numpy\n",
    "\n",
    "NOTE:\n",
    "\n",
    "- np.zeros(x, y) produces a matrix with zeros with dimensions x, y\n",
    "\n",
    "- np.random.rand(x, y) creates a random float numbers matrix with dimensions x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### CLASSIC DOT PRODUCT OF VECTORS IMPLEMENTATION ###\n",
    "tic = time.process_time()\n",
    "dot = 0\n",
    "for i in range(len(x1)):\n",
    "    dot+= x1[i]*x2[i]\n",
    "toc = time.process_time()\n",
    "print (\"dot = \" + str(dot))\n",
    "print (\"\\n ----- Computation time dot product (for) = \" + str(1000*(toc - tic)) + \"ms\")\n",
    "\n",
    "\n",
    "### CLASSIC OUTER PRODUCT IMPLEMENTATION ###\n",
    "tic = time.process_time()\n",
    "outer = np.zeros((len(x1),len(x2))) # we create a len(x1)*len(x2) matrix with only zeros\n",
    "for i in range(len(x1)):\n",
    "    for j in range(len(x2)):\n",
    "        outer[i,j] = x1[i]*x2[j]\n",
    "toc = time.process_time()\n",
    "#print (\"outer = \" + str(outer))\n",
    "print (\"\\n ----- Computation time outer prod (for)= \" + str(1000*(toc - tic)) + \"ms\")\n",
    "\n",
    "### CLASSIC ELEMENTWISE IMPLEMENTATION ###\n",
    "tic = time.process_time()\n",
    "mul = np.zeros(len(x1))\n",
    "for i in range(len(x1)):\n",
    "    mul[i] = x1[i]*x2[i]\n",
    "toc = time.process_time()\n",
    "#print (\"elementwise multiplication = \" + str(mul))\n",
    "print (\"\\n ----- Computation time elementwise (for)= \" + str(1000*(toc - tic)) + \"ms\")\n",
    "\n",
    "### CLASSIC GENERAL DOT PRODUCT IMPLEMENTATION ###\n",
    "W = np.random.rand(3,len(x1)) # Random 3*len(x1) numpy array\n",
    "tic = time.process_time()\n",
    "gdot = np.zeros(W.shape[0])\n",
    "for i in range(W.shape[0]):\n",
    "    for j in range(len(x1)):\n",
    "        gdot[i] += W[i,j]*x1[j]\n",
    "toc = time.process_time()\n",
    "#print (\"gdot = \" + str(gdot))\n",
    "print (\"\\n ----- Computation time dot prod general (for) = \" + str(1000*(toc - tic)) + \"ms\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise\n",
    "\n",
    "Look for the np methods for:\n",
    "\n",
    "1. dot product of vectors\n",
    "\n",
    "2. outer product of vectors\n",
    "\n",
    "3. elementwise multiplication\n",
    "\n",
    "4. general dot product of W (previously generated) and x1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you may have noticed, the vectorized implementation is much cleaner and more efficient. For bigger vectors/matrices, you simply **cannot compute them without numpy**. \n",
    "\n",
    "**Note** that `np.dot()` performs a matrix-matrix or matrix-vector multiplication. This is different from `np.multiply()` and the `*` operator (which is equivalent to  `.*` in Matlab/Octave), which performs an element-wise multiplication."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise: Loss and cost functions for binary target in numpy\n",
    "## i.e. as logistic regression, or those linking x and y through sigmoid function\n",
    "\n",
    "The sigmoid function in a generalized linear model computes the likelihood (ie a kind of probability) that an arbitrary output is either 0 or 1. Its formula is:\n",
    "$$ \\sigma(z^{(i)}) = \\frac{1}{(1 + e^{(-z)})}$$\n",
    "\n",
    "- The loss is used to evaluate the performance of your model. The bigger your loss is, the more different your predictions ($ \\hat{y} $) are from the true values ($y$). \n",
    "- In deep learning, you use optimization algorithms like Gradient Descent to train your model and to minimize the cost.\n",
    "- For binary targets loss is defined as:\n",
    "\n",
    "For one example $x^{(i)}$:\n",
    "$$z^{(i)} = w^T x^{(i)} + b \\tag{1}$$\n",
    "$$\\hat{y}^{(i)} = a^{(i)} = sigmoid(z^{(i)})\\tag{2}$$ \n",
    "$$ \\mathcal{L}(a^{(i)}, y^{(i)}) =  - y^{(i)}  \\log(a^{(i)}) - (1-y^{(i)} )  \\log(1-a^{(i)})\\tag{3}$$\n",
    "\n",
    "The cost is then computed by summing over all training examples:\n",
    "$$ J = \\frac{1}{m} \\sum_{i=1}^m \\mathcal{L}(a^{(i)}, y^{(i)})\\tag{6}$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# COMPUTE THE SIGMOID FUNCTION WITH NUMPY\n",
    "\n",
    "def sigmoid(z):\n",
    "    \"\"\"\n",
    "    Compute the sigmoid of z\n",
    "\n",
    "    Arguments:\n",
    "    z -- A scalar or numpy array of any size.\n",
    "\n",
    "    Return:\n",
    "    s -- sigmoid(z)\n",
    "    \"\"\"\n",
    "\n",
    "    ### START CODE HERE ### (1 line of code) REMEMBER USE np method!!!\n",
    "\n",
    "    return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print (\"sigmoid([0, 2]) = \" + str(sigmoid(np.array([0,2]))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Expected Output**: \n",
    "\n",
    "<table>\n",
    "  <tr>\n",
    "    <td>**sigmoid([0, 2])**</td>\n",
    "    <td> [ 0.5         0.88079708]</td> \n",
    "  </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ERROR FUNCTION\n",
    "\n",
    "def loss(a, y):\n",
    "    \"\"\"\n",
    "    Arguments:\n",
    "    yhat = a =  vector of size m (predicted labels)\n",
    "    y -- vector of size m (true labels)\n",
    "    \n",
    "    Returns:\n",
    "    loss -- the value of the loss function defined above **for each target element**\n",
    "    \"\"\"\n",
    "    \n",
    "    # 1 line to compute loss\n",
    "    \n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = np.array([.9, 0.2, 0.1, .4, .9])\n",
    "y = np.array([1, 0, 0, 1, 1])\n",
    "print(\"loss = \" + str(loss(a,y)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Expected Output**:\n",
    "\n",
    "<table style=\"width:20%\">\n",
    "     <tr> \n",
    "       <td> **loss** </td> \n",
    "       <td> [0.10536052 0.22314355 0.10536052 0.91629073 0.10536052] </td> \n",
    "     </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Write the cost function\n",
    "\n",
    "Once you have computed the loss function you can easily compute (using numpy) the cost function (which is not more than the average of the loss functions across the y or actual target vector).\n",
    "\n",
    "The cost can be computed by summing over all training examples:\n",
    "$$ J = \\frac{1}{m} \\sum_{i=1}^m \\mathcal{L}(a^{(i)}, y^{(i)})\\tag{6}$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FUNCTION: compute cost\n",
    "\n",
    "def cost(a, y):\n",
    "    \"\"\"\n",
    "    Computes the cost function by summing loss over all training examples.\n",
    "    \n",
    "    Arguments:\n",
    "    a -- A numpy vector or array\n",
    "    y -- A scalar or numpy vector\n",
    "    \n",
    "    You must obtain\n",
    "    m -- the size of y (use len(y))\n",
    "    \n",
    "    Return:\n",
    "    cost -- Your computed cost.\n",
    "    \"\"\"\n",
    "    \n",
    "    ### START CODE HERE ### (≈ 2 lines of code)\n",
    "    ### HINT : Define first m as a property of the input object y\n",
    "\n",
    "    ### END CODE HERE ###\n",
    "    \n",
    "    return cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "a = np.array([.9, 0.2, 0.1, .4, .9])\n",
    "y = np.array([1, 0, 0, 1, 1])\n",
    "print(\"loss = \" + str(loss(a,y)))\n",
    "print(\"cost = \" + str(cost(a,y)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# What to remember\n",
    "- Vectorization is very important in deep learning. It provides computational efficiency and clarity.\n",
    "- You have reviewed the loss and cost functions for binary targets\n",
    "- You are familiar with many numpy functions such as np.sum, np.dot, np.multiply, np.maximum, etc..."
   ]
  }
 ],
 "metadata": {
  "coursera": {
   "course_slug": "neural-networks-deep-learning",
   "graded_item_id": "XHpfv",
   "launcher_item_id": "Zh0CU"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
